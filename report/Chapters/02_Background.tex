\chapter{Background theory} \label{sec:methods}

\section{Skinned Multi-Person Linear Model (SMPL)}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/smpl.png}
    \caption{SMPL model (Image credit: \cite{SMPL:2015})}
\end{figure}
The Skinned Multi-Person Linear (SMPL) model (\cite{SMPL:2015}) is a parametric model designed to accurately represent a diverse array of human body shapes and poses. The model is constructed on the principles of Linear Blend Skinning (LBS), augmented with corrective blend shapes derived from an extensive dataset of body scans. This approach allows the SMPL model to capture the nuanced deformations associated with different body shapes and poses, ensuring high fidelity and realism in representation. Furthermore, the model is compatible with standard graphics pipelines, facilitating its easy integration and rendering.

Since its inception, the SMPL model has been extended through various enhancements. Notable among these are SMPL-H (\cite{MANO:SIGGRAPHASIA:2017}), which models detailed hand movements, and SMPL-X (\cite{SMPL-X:2019}), which extends the model to include both detailed hand movements and facial expressions. Additionally, Dynamic-SMPL or DMPL (\cite{SMPL:2015}) incorporates dynamic soft-tissue deformations, which simulate the natural movement of soft tissues during movement.

The SMPL model is parameterized by $\vec{\beta}$, which encapsulates the deviations from a mean body shape, and $\vec{\theta}$, which specifies the axis-angle rotations of the 23 joints in the template skeleton. These rotations are local and are applied using a kinematic tree structure, which ensures that the hierarchical relationships between body parts are accurately represented. Mathematically, the model can be expressed as:
\begin{equation}
    M(\vec{\beta}, \vec{\theta}) = W(T_P(\vec{\beta}, \vec{\theta}), J(\vec{\beta}), \vec{\theta}, \mathcal{W})
\end{equation}
where $T_P(\vec{\beta}, \vec{\theta})$ denotes the vertices of the rest pose, incorporating deformations due to body shape and pose, and is defined as:
\begin{equation}
    T_P(\vec{\beta}, \vec{\theta}) = \vect{\bar{T}} + B_S(\vec{\beta}) + B_P(\vec{\theta})
\end{equation}
Here, $\vect{\bar{T}}$ represents the template mesh, $B_S(\vec{\beta})$ accounts for shape-dependent deformations, and $B_P(\vec{\theta})$ represents pose-dependent deformations.

The joint locations, $J(\vec{\beta})$, are derived from the shaped template vertices using a learned regression matrix $\mathcal{J}$, and can be formulated as:
\begin{equation} \label{eq:smpl-joints}
    J(\vec{\beta}) = \mathcal{J}(\vect{\bar{T}} + B_S(\vec{\beta}))
\end{equation}
The skinning function, $W$, which can be either LBS or Dual-Quaternion Blend Skinning (DQBS), applies these deformations to the skeletal structure, with $\mathcal{W}$ representing the blend weights.

\section{Human Motion Estimation}
% TODO: Write about HuMoR

\section{Tranformer architecture}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{figures/transformer.png}
    \caption{The transformer (Image from \cite{transformer2017}.)}
    \label{fig:transformer}
\end{figure}
Ever since its publication, the Transformer (\cite{transformer2017}) has revolutionized the field of deep learning. Despite being initially designed for Natural Language Processing (NLP), Transformers have demonstrated their versatility and effectiveness across a wide range of domains. Beyond NLP, Transformers have been successfully applied to other areas such as computer vision, where models like Vision Transformers (ViTs) (\cite{dosovitskiy2020vit}) leverage the same principles to process image data. In addition, fields such as speech recognition (\cite{conformer2020}), time-series forecasting (\cite{haoyietal-informer-2021}), and even protein folding (\cite{Jumper2021}) have seen breakthroughs thanks to the adoption of Transformer-based models. 

In contrast to models such as RNNs, GRUs and LSTMs relying on sequential processing of hidden states, the transformer model utilizes an attention mechanism for communication. The attention mechanism enables the model to capture long-range dependencies more effectively and has significantly improved the performance of various NLP tasks, including translation, summarization, and question answering (\cite{devlin2018bert}). Furthermore, due to the parallel nature of attention, the Transformer architecture has been subject for huge scaling with models exceeding billion of parameters (\cite{radford_language_2019}, \cite{touvron2023llama2openfoundation}, \cite{anil2023palm2technicalreport}). 

The architecture comprises an encoder and a decoder as illustrated in~\cref{fig:transformer}. The encoder uses multi-headed self-attention to create semantic representations for each element in the input sequence, allowing the model to process the entire context simultaneously. Through cross-attention, the decoder integrates information from the encoder's output, while causal self-attention ensures that the model only considers previously generated tokens by masking out future tokens in the training phase. This enables autoregressive generation at inference time. 

The transformer's attention mechanism is based on computing the scaled dot-product: 
\begin{equation}
    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}} \right) \mathbf{V},
\end{equation}
where $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ are the query, key, and value matrices, respectively, and $d_k$ is the dimension of the key vectors. Hence, scaled dot-product attention calculates the attention score between all query and key vectors to identify the corresponding value vectors where the key-query alignment is strong. The softmax function and normalization constant, $d_k$, ensure that the attention output maintains an appropriate scale. However, one might want to allow the model to attend to multiple items. Multi-headed attention achieves this by using multiple attention mechanisms in parallel, each focusing on different parts of the input. Mathematically, it is formulated as:
\begin{equation}
    \begin{aligned}
        & \text{MultiHead}(\vect{Q}, \vect{K}, \vect{V})=\operatorname{Concat}\left(\text{head}_1, \ldots, \text{head}_{\mathrm{h}}\right) \vect{W}^O \\
        & \text {where head}_{\mathrm{i}}=\operatorname{Attention}\left(\vect{Q} \vect{W}_i^Q, \vect{K} \vect{W}_i^K, \vect{V} \vect{W}_i^V\right) \\
        &
    \end{aligned}
\end{equation}
where $\vect{W}^O \in \mathbb{R}^{h d_v \times d_\text{model}}$ is the output projection matrix and $\vect{W}_i^Q, \vect{W}_i^K \in \mathbb{R}^{d_\text{model} \times d_k}$, $\vect{W}_i^V \in \mathbb{R}^{d_\text{model} \times d_k}$ and $W_o \in \mathbb{R}^{h d_v \times d_\text{model}}$ are linear projection matrices reducing the dimensionality to $d_k = d_v = d_\text{model} / h$ where $h$ is the number of heads. In other words, the query, key and value vectors are projected down to multiple heads between which the attention is computed before being concatenated and finally reprojected into the model dimensions. 

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.6\linewidth}
        \includegraphics[width=\linewidth]{figures/positional_encoding.pdf}
        \caption{Sinusoidal positional encoding with values from $1$ (white) to -1 (black). $d_\text{model}$ = 196 and max length, $L=64$.}
        \label{fig:positional-encoding}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.35\linewidth}
        \includegraphics[width=\linewidth]{figures/positional_encoding_similarity.pdf}
        \caption{Normalized dot-product of encodings at positions $i$ and $j$.}
        \label{fig:positional-encoding-similarity}
    \end{subfigure}
\end{figure}
Since the transformer architecture does not naturally encode positional information, \cite{transformer2017} introduce an additive sinusoidal encoding (\cref{fig:positional-encoding}) to the model's input embeddings:
\begin{equation}
    \textbf{PE}(i, \delta)= \begin{cases}\sin \left(\frac{i}{10000^{2 \delta^{\prime} / d_\text{model}}}\right) & \text { if } \delta=2 \delta^{\prime} \\ \cos \left(\frac{i}{10000^{2 \delta^{\prime} / d_\text{model}}}\right) & \text { if } \delta=2 \delta^{\prime}+1\end{cases}.
\end{equation}
for $\delta=1,\ldots,d_\text{model}$ and $i = 1,\ldots,L$. Considering the the query-key matrix dot-product with added positional encodings:
\begin{equation}
    \left(\vect{Q} + \textbf{PE} \right) \left( \vect{K} + \textbf{PE}  \right)^T = \vect{Q} \vect{K}^T+\vect{Q} \textbf{PE}^T+\textbf{PEK}^T+\textbf{PEPE}^T. \quad \includegraphics[width=5mm]{figures/pepe.jpg}
\end{equation}
This expansion reveals the various interactions between the query, key, and positional encodings. Specifically, the term $\vect{Q} \vect{K}^T$ computes the standard attention score based on the query and key. The term $\vect{Q} \textbf{PE}^T$ captures the interaction between the content of the query and positional encodings, while $\textbf{PE} \vect{K}^T$ captures the interaction between the positional encodings and the content of the key. Finally, $\textbf{PE} \textbf{PE}^T$ represents the self-interaction of the positional encodings. An example of this is illustrated in~\cref{fig:positional-encoding-similarity} showing the normalized dot-product similarity of positional encodings at relative distances. These combined interactions enable the model to learn to attend to items at both absolute and content-dependent offsets, relavant for the objective at hand. 

\section{Denoising Diffusion Probabilistic Models (DDPM)}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/pgm_diagram_xarrow_small.pdf}
    \caption{Diffusion forward and backward process (taken from \cite{ho2020denoising})}
    \label{fig:diffusion-process}
\end{figure}
In recent years, several types of generative models such as Variational Autoencoders (VAEs), Generative Adverserial Networks (GANs), autoregressive models and flow-based models have shown remarkable results in data generation of varying data modalities, such as images, audio, videos and text. Most recently, Denoising Diffusion Probabilistic Models (DDPMs) have gained large popularity especially within the field of image generation due to several reasons such as high-quality data generation, versitility in several data domains as well as controllability, allowing one to steer the generation towards desired outputs.

A DDPM is a parametarized Markov chain trained using variational inference to reverse a (forward) diffusion process, $q(\vect{x}_{1:T} \mid \vect{x}_0)$, as seen in \cref{fig:diffusion-process} wherein the signal of the data, $\vect{x}_0$, is gradually destroyed by adding gaussian noise according to predefined noise schedule $\{\beta_t  \in (0,1) \}_{t=1}^T$ giving rise to increasingly noisy samples, $\vect{x}_1 \ldots \vect{x}_T$. 

The forward process is defined as follows:
\begin{equation}    
    q(\vect{x}_{1:T} \mid \vect{x}_{0}) = \prod_{t=1}^T q(\vect{x}_{t} \mid \vect{x}_{t-1}), \quad q(\vect{x}_{t} \mid \vect{x}_{t-1}) = \mathcal{N}(x_{t}; \sqrt{1 - \beta_t}\vect{x}_t,\beta_t \vect{I} )
    \label{eq:diffusion}
\end{equation}
with $T$ being the discritized number of diffusion steps before all original information is completely discarded. The goal of the inverse or backwards process then becomes to iteratively remove the noise, in order to arrivate at the original data. More formally, the process is defined as:
\begin{equation}
    p_\theta(\vect{x}_{0:T}) = p(\vect{x}_T) \prod_{t=1}^T p_\theta(\vect{x}_{t-1} \mid x_{t}), \quad p_\theta(\vect{x}_{t-1} \mid \vect{x}_{t}) = \mathcal{N}\left(\vect{x}_{t-1}; \vect{\mu}_\theta(\vect{x}_t, t), \vect{\Sigma}_\theta(\vect{x}_t, t) \right)
\end{equation}
taking starting point in pure noise $p(\vect{x}_T) = \mathcal{N}(\vect{x}_T; \vect{0}, \vect{I})$, incrementally removing the noise through the learned functions, $\vect{\mu}_\theta(\vect{x}_t, t)$ and $\vect{\Sigma}_\theta(\vect{x}_t, t)$ commonly parameterized by a deep neural network.
Using the reparameterization trick, we are able to sample any noisy version of our data, $\vect{x}_t$, at time step $t$ given our original data $\vect{x}_0$. Recall our forward transition probability function, $q(\vect{x}_t \mid \vect{x}_{t-1})$. Letting $\alpha_t = 1 - \beta_t$, $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$ and using the reparameterization trick the expression can be rewritten as:
\begin{equation}
    \vect{x}_t = \sqrt{\alpha_t} \vect{x}_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1}
\end{equation}
where $\epsilon_{t-1} \sim \mathcal{N}(0,1)$. Expanding the recursive definition then gives:
\begin{align*}
    \vect{x}_t    % & = \sqrt{\alpha_t} \vect{x}_{t-1} + \sqrt{1 - \alpha_t} \vect{\epsilon_{t-1}} \\
                    & = \sqrt{\alpha_t} \left( \sqrt{\alpha_{t-1}} \vect{x}_{t-2} + \sqrt{1 - \alpha_{t-1}} \vect{\epsilon}_{t-2} \right) + \sqrt{1 - \alpha_t} \vect{\epsilon_{t-1}} \\
                    % & = \sqrt{\alpha_t \alpha_{t-1}} \vect{x}_{t-2} + \sqrt{\alpha_t(1 - \alpha_{t-1})} \vect{\epsilon_{t-2}} + \sqrt{1 - \alpha_{t}} \vect{\epsilon_{t-1}} \\
                    & = \sqrt{\alpha_t \alpha_{t-1}} \vect{x}_{t-2} + \sqrt{1 - \alpha_{t}\alpha_{t-1}} \vect{\bar{\epsilon}}_{t-2}
\end{align*}
where $\vect{\bar{\epsilon}}_{t-2}$ merges the two independent Gaussians $\vect{\epsilon}_{t-1}$ and $\vect{\epsilon}_{t-2}$ into a single Gaussian with new variance as the sum of variances $\alpha_t (1 - \alpha_{t-1}) + (1 - \alpha_{t}) = 1 - \alpha_t \alpha_{t-1}$. Recursively applying the definition of $\vect{x}_t$ and merging the gaussian noise terms results in the simplified expression:
\begin{equation} \label{eq:nice}
    \vect{x}_t = \sqrt{\bar{\alpha}_t}\vect{x}_0 + \sqrt{1 - \bar{\alpha}_t} \vect{\epsilon}, \quad \vect{\epsilon} \sim \mathcal{N}(\vect{0}, \vect{I})
\end{equation}
% or by undoing the reparameterization:
% \begin{equation}
%     q(\vect{x}_t \vert \vect{x}_0) = \mathcal{N}(\vect{x}_t; \sqrt{\bar{\alpha}_t} \vect{x}_0, (1 - \bar{\alpha}_t)\vect{I})
% \end{equation}
%This result enables sampling of any noisy version of $\vect{x}_t$ at time $t$ given "clean" data $\vect{x}_0$. 
Conversely, given $\vect{x}_0$, the reverse conditional probability:
\begin{equation*}
    q(\vect{x}_{t-1} \vert \vect{x}_t, \vect{x}_0) = \mathcal{N}\left( \vect{x}_{t-1}; \vect{\tilde{\mu}}_t(\vect{x}_t, \vect{x}_0), \tilde{\beta}_t \vect{I} \right)
\end{equation*}
becomes tractable to compute using Bayes rule (derivation in \cref{appendix:q-posterior-derivation}) with mean and variance given by:
\begin{align} \label{eq:x-t-min-1-from-x-t}
    \vect{\tilde{\mu}}_t(\vect{x}_t, \vect{x}_0) &= \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \vect{x}_0+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \vect{x}_t \overset{\text{Using \ref{eq:nice}}}{=} \frac{1}{\sqrt{\alpha_t}}\left(\vect{x}_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \vect{\epsilon}_t\right) \\
    \tilde{\beta}_t &= \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t
\end{align}
The model is trained by optimizing the variational lower bound on the log likelihood:
\begin{equation}
    L_{VLB}
        = \mathbb{E}_q\left[ \log \frac{p_\theta(\vect{x}_{0:T})}{q(\vect{x}_{1:T} \vert \vect{x}_0)}\right]
        = \mathbb{E}_q\left[\log p(\vect{x_T}) + \sum_{t \geq 1} \log \frac{p_\theta(\vect{x}_{t-1} \vert \vect{x}_{t})}{q(\vect{x}_t \vert \vect{x}_{t-1})}
        \right]
        \leq \mathbb{E}\left[\log p_\theta(\vect{x}_0)\right]
\end{equation}
which in practice means minimizing the negative variational lower bound. 
\cite{ho2020denoising} rewrites this into a sum of KL-divergences:
\begin{equation}
    L_{VLB} = \mathbb{E}_q \left[ \underbrace{D_{\mathrm{KL}} \left(
            q\left(\vect{x}_T \mid \vect{x}_0\right) \| p\left(\vect{x}_T\right)
            \right)}_{L_T} + \sum_{t=2}^T \underbrace{D_{\mathrm{KL}}\left(q\left(\vect{x}_{t-1} \mid \vect{x}_t, \vect{x}_0\right) \| p_\theta\left(\vect{x}_{t-1} \mid \vect{x}_t\right)\right)}_{L_{t-1}} - \underbrace{\log p_\theta\left(\vect{x}_0 \mid \vect{x}_1\right)}_{L_0} \right]
\end{equation}
where the authors model $L_0$ from seperate discrete decoder. As $L_T$ doesn't depend on our parameters, $\theta$, and is therefore constant, it can be ignored during optimization. The rest of the $L_{t-1}$ terms can be efficiently computed in the closed form, by fixing the variance $\vect{\Sigma}_\theta(\vect{x}_t, t) = \sigma_t^2 \vect{I}$ to only depend on the current timestep (authors propose $\sigma_t^2 = \beta_t$ or $\sigma_t^2 = \tilde{\beta}_t$). $L_{t-1}$ can then be written in closed form (derivation in \cref{appendix:L-closed-form}) as:
\begin{equation}
    L_{t-1} = \mathbb{E}_q \left[ \frac{1}{2\sigma_t^2} \| \vect{\tilde{\mu}}_t(\vect{x}_t, \vect{x}_0) - \vect{\mu}_\theta(\vect{x}_t, t) \|^2 \right] + C_t
\end{equation}
where $C_t$ is a constant depending on the choice of $\sigma_t^2$ and the noise schedule. Using \cref{eq:x-t-min-1-from-x-t} \cite{ho2020denoising} reparameterize the expression in terms of predicting the noise:
\begin{equation}
    L_{t-1} = \mathbb{E}_{\vect{x}_0, \vect{\epsilon}}\left[
        \frac{\beta_t^2}{2 \sigma_t^2 \alpha_t \left( 1 - \bar{\alpha}_t\right)} \left\| 
            \vect{\epsilon}-\vect{\epsilon}_\theta \left(\sqrt{\bar{\alpha}_t} \vect{x}_0+\sqrt{1-\bar{\alpha}_t} \vect{\epsilon}, t\right)
            \right\|^2
    \right]
\end{equation}
as they find it leads to better unconditional sample quality when training on the CIFAR10 dataset. Furthermore, they report the best sample quality when using the "simple" objective, ignoring the weighing:
\begin{equation} \label{eq:ddpm-simple-objective}
    L_\mathrm{simple} = \mathbb{E}_{t \sim \mathcal{U}(1, T),\ \vect{\epsilon}} \left[
        \vect{\epsilon} - \vect{\epsilon}_\theta( \sqrt{\bar{\alpha}} \vect{x}_0 + \sqrt{1 - \bar{\alpha}}\vect{\epsilon}, t)
    \right]
\end{equation}

The final training and sampling scheme, as outlined in \cref{alg:training} and \cref{alg:sampling} can be summarized as follows. The training algorithm optimizes the model to predict and remove noise from data, while the sampling algorithm uses the trained model to iteratively transform random noise into structured data.

\algrenewcommand\algorithmicindent{0.5em}%
\begin{figure}[t]
\begin{minipage}[t]{0.495\textwidth}
\begin{algorithm}[H]
  \caption{Training} \label{alg:training}
  \small
  \begin{algorithmic}[1]
    \Repeat
      \State $\vect{x}_0 \sim q(\vect{x}_0)$
      \State $t \sim \mathrm{Uniform}(\{1, \dotsc, T\})$
      \State $\vect{\epsilon}\sim\mathcal{N}(\vect{0},\vect{I})$
      \State Take gradient descent step on
      \Statex $\qquad \grad_\theta \left\| \vect{\epsilon} - \vect{\epsilon}_\theta(\sqrt{\bar\alpha_t} \vect{x}_0 + \sqrt{1-\bar\alpha_t}\vect{\epsilon}, t) \right\|^2$
    \Until{converged}
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.495\textwidth}
\begin{algorithm}[H]
  \caption{Sampling} \label{alg:sampling}
  \small
  \begin{algorithmic}[1]
    \vspace{.04in}
    \State $\vect{x}_T \sim \mathcal{N}(\vect{0}, \vect{I})$
    \For{$t=T, \dotsc, 1$}
      \State $\vect{z} \sim \mathcal{N}(\vect{0}, \vect{I})$ if $t > 1$, else $\vect{z} = \vect{0}$
      \State $\vect{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left( \vect{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}} \vect{\epsilon}_\theta(\vect{x}_t, t) \right) + \sigma_t \vect{z}$
    \EndFor
    \State \textbf{return} $\vect{x}_0$
    \vspace{.04in}
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-1em}
\end{figure}

% To summarize, diffusion models are generative models that create data by reversing a diffusion process. They are trained using a simplified version of the variational lower bound on the log-likelihood. Starting from pure noise, the model iteratively removes the predicted noise through a step-by-step denoising process, effectively reconstructing the original data. This approach allows DDPMs to produce high-quality and diverse outputs.

\subsection{Variance schedules}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/schedule.pdf}    
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/snr.pdf}    
    \end{subfigure}
    \caption{Linear and cosine schedule and signal-to-noise ratio.}
    \label{fig:schedule}
\end{figure}
In the DDPM paper \cite{ho2020denoising} choose a variance schedule with $\beta_t$ linearly increasing from $\beta_1 = 10^{-4}$ to $\beta_T = 0.02$. However, this results in $\vect{x}_t$ almost entirely losing its signal in the last quarter of the schedule as problematized by \cite{pmlrv139nichol21a}. The authors instead propose a cosine variance schedule, where the squared signal proportion is given by:
\begin{equation}
    \bar{\alpha}_t=\frac{f(t)}{f(0)}, \quad f(t)=\cos \left(\frac{t / T+s}{1+s} \cdot \frac{\pi}{2}\right)^2
\end{equation}
where $s$ controls the offset of the noise schedule. The authors set $s=0.008$ as they found that $s=0$ resulted in minuscule noise near $t=0$ making it hard for the model to predict $\epsilon$. From $\bar{\alpha}_t$ one can then compute $\beta_t = \min\left( 1 - \frac{\bar{\alpha}_t}{\bar{\alpha}_{t-1}}, 0.999 \right)$ with the $\min$ to prevent singularities. A comparison of the cosine and linear schedules can be seen in \cref{fig:schedule}. 


\subsection{Classifier and Classifier-Free Guidance (CFG)}
In practice, it is often desirable to steer the generation process in order to output data belonging to a specific class, such as cats or dogs in the context of image generation, or incorporating some other information relevant for the output. This process can be framed within the context of probability theory as conditional generation, where the aim is to maximize the probability of the data, $\vect{x}$, given some conditioning signal, $y$, (e.g. a class label. According to Bayes' rule, the conditional probability can be expressed as:
\begin{equation}
    p(\vect{x} \mid y) = \frac{p(\vect{x}, y)}{p(y)} \propto p(y \mid \vect{x}) p(\vect{x})
\end{equation}
Hence, it is proportional to the joint probability of the data and label, which from basic probability theory is equal to the product of the unconditional probability of the data, $p(\vect{x})$, and the probability of the conditioning signal given the data, $p(y \mid \vect{x})$ (for labels; that the data belonging to the given class). In the DDPM paper \cite{ho2020denoising} establishes connection between diffusion models and Noise-Conditioned Score Networks (NCSN) and shows how the reverse diffusion process can be seen as a temporal discritized type of annealed Langevin dynamics given by the stochastic differential equation:
\begin{equation}
    \frac{d}{dt} \log p(\vect{x}_t) = \sqrt{\bar{\alpha}_t} s(\vect{x}_t, t) + \sqrt{1 - \bar{\alpha}_t} \vect{z}_t
\end{equation}
where $z_t \sim \mathcal{N}(0, 1)$ and $s(\vect{x}_t, t) = \grad_{\vect{x}_t} \log p(\vect{x}_t)$ is referred to as the score function. It turns out that the denoising network can be used to approximate the score function:
\begin{equation}
    s(\vect{x}_t, t) \approx -\frac{1}{\sqrt{1 - \bar{\alpha}_t}} \vect{\epsilon}_\theta(\vect{x}_t, t)
\end{equation}
Returning to the factorization of the conditional probability, taking the gradient of the log of both sides reveals the relationship with the score function:
\begin{equation}
    \grad_{\vect{x}_t} \log p(\vect{x}_t \mid y ) = \grad_{\vect{x}_t} \log p(y \mid \vect{x}_t) + \underbrace{\grad_{\vect{x}_t} \log p(\vect{x}_t)}_{s(\vect{x}_t, t)}
\end{equation}
\cite{dharial2021diffusion} trains a classifier, $f_\phi(y \mid \vect{x}_t) \approx p(y \mid \vect{x}_t)$, seperately to predict the class label of noisy images. By using the reformulated denoising function:
\begin{equation} \label{eq:classifier-guidance}
    \bar{\vect{\epsilon}}_{\theta}(\vect{x}_t, t) = \vect{\epsilon}_\theta(\vect{x}_t, t) - \sqrt{1 - \bar{\alpha}_t} \grad_{\vect{x}_t} \log f_\phi(y \mid \vect{x}_t)
\end{equation}
where $s$ controls guidance strength, it is possible to guide the diffusion sampling e.g. in order to generate images of a specific category, hence the name classifier guidance. 

In contrast to classifier guidance, an alternative method known as classifier-free guidance, proposed by \cite{ho2021classifierfree}, eliminates the need for a separate guidance network. This method leverages the same generative model to perform conditional generation by incorporating the conditioning signal, $y$, during the training process. Specifically, the model is trained with the conditioning signal but occasionally discards it with a certain probability, $p_\text{uncond}$. This technique allows the model to learn both the conditional and unconditional generation processes.

During the forward diffusion process, the model is trained to predict the noise added to the data both conditionally, $\vect{\epsilon}_\theta(\vect{x}_t, t, y)$, and unconditionally, $\vect{\epsilon}_\theta(\vect{x}_t, t, y = \emptyset)$. The conditional prediction uses the conditioning signal $y$ (e.g., class labels), while the unconditional prediction is made without any conditioning.

At inference time, classifier-free guidance combines these predictions to steer the generation process towards the desired condition. This is achieved by interpolating between the conditional and unconditional predictions:
\begin{equation}
    \bar{\vect{\epsilon}}_\theta(\vect{x}_t, t, y) = \vect{\epsilon}_\theta(\vect{x}_t, t, y = \emptyset) + s \cdot (\vect{\epsilon}_\theta(\vect{x}_t, t, y) - \vect{\epsilon}_\theta(\vect{x}_t, t, y = \emptyset))
    \label{eq:classifer-free}
\end{equation}
Here, $s$ is a scaling factor that controls the strength of the guidance. This formulation allows the model to flexibly adjust the influence of the conditioning signal on the generation process, effectively guiding the reverse diffusion towards samples that match the desired condition.

By leveraging the same network for both conditional and unconditional noise predictions, classifier-free guidance simplifies the implementation and improves the efficiency of the conditional generation process. This method has demonstrated significant improvements in generating high-quality samples (\cite{ho2021classifierfree}) that adhere to the specified conditions, making it a powerful alternative to traditional classifier guidance approaches.




\section{Human Motion Diffusion}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/mdm_comb.pdf}
    \caption{\textbf{(Left)} MDM architecture. \textbf{(Right)} MDM Sampling process. (Image taken from \cite{tevet2023human})}
    \label{fig:mdm}
\end{figure}
The Motion Diffusion Model (MDM) (\cite{tevet2023human}) provides a generative model for single human motion conditioned on natural language descriptions using classifier-free guided diffusion models. The authors model human motion as a sequence of poses, $x^{i} \in \mathbb{R}^{J \times D}$, represented by either their joint rotations or positions where $J$ is the number of joints and $D$ is the dimensions of the representation and $i = 1, \ldots, N$ is the current frame of the motion. Other examples of pose representations include that of the HumanML3D dataset (\cite{Guo_2022_CVPR}):
\begin{equation}
    \vect{x}^{(i)} = \left(\dot{r}^a, \dot{r}^x, \dot{r}^z, r^y, \vect{j}^p, \vect{j}^v, \vect{j}^r, \vect{c}^f\right)
    \label{eq:hml3d-repr}
\end{equation}
where $\dot{r}^a \in \mathbb{R}$ is the angular velocity along the Y-axis; $(\dot{r}^x, \dot{r}^z)$ linear velocities in the XZ-plane; $r^y \in \mathbb{R}$ the height; $\vect{j}^p \in \mathbb{R}^{3J}$, $\vect{j}^v \in \mathbb{R}^{3J}$ and $\vect{j}^r \in \mathbb{R}^{6J}$ the joint positions, velocities and 6D rotations all in the local coordinate system and finally $\vect{c}^f \in \mathbb{R}^{4}$ the binary foot ground contact indicators obtained from thresholding the feet velocities and heights.

Through the diffusion process:
\begin{equation}
    q(\vect{x}_t^{(1:N)} \mid \vect{x}_{t-1}^{(1:N)}) = \mathcal{N}\left( \sqrt{\alpha_t} \vect{x}_{t-1}^{(1:N)} \mid (1 - \alpha_t) \vect{I} \right),
\end{equation} 
the motion sequence is gradually degraded until becoming pure noise $x_T^{1:N} \sim \mathcal{N}(0, 1)$ at the final timestep $t=T$ of the noise schedule, in correspondence with \cref{eq:diffusion}. To reverse the diffusion process, the authors suggest parameterizing the score function, $G(\vect{x}_t^{(1:N)}, t, c)$ to predict the signal instead of the noise, where $c$ is the conditioning signal. In addition to the \textit{simple} loss function given by:
\begin{equation}
    \mathcal{L}_{\text {simple }}=E_{x_0 \sim q\left(x_0 \mid c\right), t \sim[1, T]}\left[\left\|x_0-G\left(x_t, t, c\right)\right\|_2^2\right]
\end{equation}
the authors introduce several geometric loss functions. This aligns with previous work on motion generative models (\cite{petrovich2021actionconditioned3dhumanmotion}, \cite{Shi_2020}) to help enforce physical constraints necessary for realistic motion, preventing artifacts such as teleportation and foot sliding. These include joint velocities and foot ground contact:
\begin{equation}
    \begin{gathered}
        \mathcal{L}_{\mathrm{pos}}=\frac{1}{N} \sum_{i=1}^N\left\|F K\left(x_0^i\right)-F K\left(\hat{x}_0^i\right)\right\|_2^2 \\
        \mathcal{L}_{\text {foot }}=\frac{1}{N-1} \sum_{i=1}^{N-1}\left\|\left(F K\left(\hat{x}_0^{i+1}\right)-F K\left(\hat{x}_0^i\right)\right) \cdot f_i\right\|_2^2 \\
        \mathcal{L}_{\text {vel }}=\frac{1}{N-1} \sum_{i=1}^{N-1}\left\|\left(x_0^{i+1}-x_0^i\right)-\left(\hat{x}_0^{i+1}-\hat{x}_0^i\right)\right\|_2^2
    \end{gathered}
\end{equation}
where $FK$ is the forward-kinematic function in case joint rotation representions and the identity function otherwise and $f_i \in \{ 0, 1 \}$ is the binary foot contact indicators, punishing non-zero foot velocities when in contact with the ground. Hence, the final loss object becomes:
\begin{equation}
    \mathcal{L} = \mathcal{L}_\text{simple} + \lambda_\text{pos} \mathcal{L}_\text{pos} + \lambda_\text{vel} \mathcal{L}_\text{vel} + \lambda_\text{foot} \mathcal{L}_\text{foot}.
\end{equation}

To guide the model in generating motion consistent with the input text description, the authors use classifier-free guidance where the conditioning signal is dropped $p=10\%$ of the time during training by setting $c=\emptyset$ and applied when guiding the sampling process as in \cref{eq:classifer-free}. The authors use a frozen \textit{CLIP-ViT-B/32} model (\cite{radford2021learning}) for constructing the description vector embeddings used as conditioning signal.

The MDM denoising network utilizes an encoder transformer, as illustrated in \cref{fig:mdm}. This network receives three inputs: the CLIP description embedding ($c$), the current diffusion timestamp ($t$), and the embedded pose representations ($\vect{x}_t^{(i)}$). Both $c$ and $t$ are linearly projected into $d_\text{model}$ dimensions and subsequently combined. Each pose representation in the motion sequence is also linearly embedded into $d_\text{model}$ dimensions. To capture the sequential relationships between poses, a sinusoidal positional encoding is added to these embeddings encoding their respective frame indices.

% Talk about InterGen

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/intergen-dm-ro.pdf}
    \caption{Illustration of Distance Map (right), floor-distance threshold (gray cylinder) and Relative Orientation (cyan and magenta arrows). (Image credit: \cite{liang2024intergen})}
    \label{fig:intergen-losses}
\end{figure}

While single human motion generation has numerous use cases, certain applications necessitate generation of multi-human motion, which requires a comprehensive understanding of human interactions. InterGen (\cite{liang2024intergen}) expands upon the work of MDM by extending the single human motion diffusion model to accommodate two-person motion generation. The authors exploit that symmetric property of multi-person motion, $p(\vect{x}^{(a)}, \vect{x}_t^{(b)}) \equiv p(\vect{x}^{(b)}, \vect{x}^{(a)}$ yielding the score function:
\begin{align}
    \nabla_{\vect{x}_t} \log p_t\left(\vect{x}_t\right) &= \left[
        \nabla_{\vect{x}_t^{(a)}} \log p_t\left(\vect{x}_t^{(a)}, \vect{x}_t^{(b)}\right), 
        \nabla_{\vect{x}_t^{(b)}} \log p_t\left(\vect{x}_t^{(b)}, \vect{x}_t^{(a)}\right)
    \right] \\
    & = \left[
        \nabla_{\vect{x}_t^{(b)}} \log p_t\left(\vect{x}_t^{(b)}, \vect{x}_t^{(a)}\right),
        \nabla_{\vect{x}_t^{(a)}} \log p_t\left(\vect{x}_t^{(a)}, \vect{x}_t^{(b)}\right) 
    \right],
\end{align}
by sharing the weights, $\theta$, between the two denoising networks $D_\theta^{(a)}$ and $D_\theta^{(b)}$, each cooperatively restoring their respective motions, $\vect{x}^{(a)}$ and $\vect{x}^{(b)}$ by integrating the hidden states, $\vect{h}^{(b)}, \vect{h}^{(a)}$, of the other network though a cross-attention mechanism. Though the motion representation in \cref{eq:hml3d-repr} is overparameterized and neural network friendly, it fails to capture the inter-person distances as joints are anchored to the local coordinate system. InterGen instead represents the joint positions and velocities in the global coordinate system:
\begin{equation}
    \vect{x}^{(i)} = \left( \vect{j}_g^p, \vect{j}_g^v, \vect{j}^r, \vect{c}^f \right),
\end{equation}
where $\vect{j}_g^p, \vect{j}_g^v \in \mathbb{R}^{3J}$ are the global joint positions and velocities while $\vect{j}^r \in \mathbb{R}^{6J}$ and $\vect{c}^f \in \mathbb{R}^4$ remains the same 6D local joint rotations and foot ground contact indicators as in the HumanML3D representation. Besides the previously introduced joint position, velocity and foot geometric losses, InterGen includes additional bone length (BL), masked joint distance map (DM) and relative orientation (RO) losses given by:
\begin{gather}
    \mathcal{L}_\text{BL}=\left\|B\left(\hat{\vect{x}}_a\right)-B\left(\vect{x}_a\right)\right\|_2^2+\left\|B\left(\hat{\vect{x}}_b\right)-B\left(\vect{x}_b\right)\right\|_2^2  \\
    \mathcal{L}_\text{DM} = \left\|\left(M\left(\hat{\mathbf{x}}_a, \hat{\mathbf{x}}_b\right)-M\left(\mathbf{x}_a, \mathbf{x}_b\right)\right) \odot I\left(M_{x z}\left(\mathbf{x}_a, \mathbf{x}_b\right)<\bar{M}\right)\right\|_2^2 \\
    \mathcal{L}_\text{RO} = \left\|O\left(I K\left(\hat{\mathbf{x}}_a\right), I K\left(\hat{\mathbf{x}}_b\right)\right)-O\left(I K\left(\mathbf{x}_a\right), I K\left(\mathbf{x}_b\right)\right)\right\|_2^2
\end{gather}
to help the model learn the interdependent relationships of the combined motion, where $B$ calculates the bone-lengths, $M(\vect{x}^{(a)}, \vect{x}^{(b)})$ the pairwise bipartiate distance map between each joint in the two poses, thresholded by the the floor-distance $M_{xz}$ being less than $\hat{M}$ (authors use 1m in their implementation). A visual interpretation is illustrated in \cref{fig:intergen-losses}. 

These geometric losses are all combined into a regularization loss term:
\begin{equation}
    \mathcal{L}_\text{reg} = \lambda_\text{vel} \mathcal{L}_\text{vel} + \lambda_\text{foot} \mathcal{L}_\text{foot} + \lambda_\text{BL} \mathcal{L}_\text{BL} + \lambda_\text{RO} \mathcal{L}_\text{RO}
\end{equation}
where $\lambda_\text{vel}$, $\lambda_\text{foot}$, $\lambda_\text{BL}$ and $\lambda_\text{RO}$ weighs the relative importance of each loss term. Hence, the the overall loss objective becomes:
\begin{equation}
    \mathcal{L} = \mathcal{L}_\text{simple} + \lambda_\text{reg} \mathbb{E}_t \left[ I(t \leq \bar{T}) \mathcal{L}_\text{reg} \right]
\end{equation}
with regularization loss term masked out for $t < \bar{T}$ as the authors find it yields better results in terms of motion quality and diversity. Intuitively, this can be understood as not "punishing" the model too much early in the denoising phase where joint positions are still relatively noisy and subject to change.
