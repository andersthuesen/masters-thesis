\documentclass[options]{report}

\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{amsmath}

\title{{\large Master Thesis} \\ 3D human interaction synthesis \\ for action recognition data augmentation}


\newcommand{\norm}[2]{\left \lVert #1 \right \rVert_{#2}}

\author{Anders Bredgaard Thuesen}
\date{\today}


\begin{document}
\maketitle

\section*{Abstract}

\section*{Introduction}



\section*{Methods}

\subsection*{SMPL \& Human Mesh Reconstruction }
The Skinned Multi-Person Linear (SMPL) model is a parametric body shape model that accurately represents a wide range of human bodies and poses. It is built upon a foundation of linear blend skinning enhanced with corrective blend shapes, which are derived from a large dataset of body scans. The model captures the subtle deformations that occur with different body shapes and poses and can easily be rendered due to its compatibility with existing graphics pipelines. Since its publication, several extensions such as DMPL, incorporating dynamic soft-tissue deformation and SMPL-X, also modelling hands and facial expressions have been introduced. The model is parameterized by $\vec{\beta}$, capturing the variations from a mean body shape and $\vec{\theta}$, specifying the axis-angle rotation of 23 of the template skeleton joints. Mathematically, the model can be expressed as:
\begin{equation}
    M(\vec{\beta}, \vec{\theta}) = W(T_P(\vec{\beta}, \vec{\theta}), J(\vec{\beta}), \vec{\theta}, \mathcal{W})
\end{equation}
where $T_P(\vec{\beta}, \vec{\theta})$ returns the vertices of the rest pose, incorporating the deformations from the body shape and pose and is given by:
\begin{equation}
    T_P(\vec{\beta}, \vec{\theta}) = \mathbf{\bar{T}} + B_S(\vec{\beta}) + B_P(\vec{\theta})
\end{equation}
$J(\vec{\beta})$ returns the 3D joint locations from the shaped template vertices using a learned regression matrix $\mathcal{J}$ and is given by:
\begin{equation}
    J(\vec{\beta}) = \mathcal{J}(\mathbf{\bar{T}} + B_S(\vec{\beta}))
\end{equation}
$W$ is the skinning function (e.g. Linear Blend Skinning (LBS) or Dual-Quaternion Blend Skinning (DQBS)) and $\mathcal{W}$ is the blend weights. 


\subsection*{Tracking \& Matching}
To track the prediction over time we naively compare each prediction point-wise with the latest track of people in the scene according to the following loss function;
\begin{equation}
    \mathcal{L}_\text{match}(a, b) = \alpha \norm{a_\text{3D kpts} - b_\text{3D kpts}}{2} + \beta \norm{a_\text{class} - b_\text{class}}{\infty},
\end{equation}
incorporating both the Euclidean distance between the 3D joint keypoints as well as the predicted patient class, with $\alpha$ and $\beta$ weighting the influence of each term. The predictions are then greedily assigned to the tracks and new tracks are created for unassigned detections. 




\end{document}